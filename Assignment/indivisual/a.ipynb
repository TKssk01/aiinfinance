{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# テキストファイルからデータを読み込む関数\n",
    "def read_txt_file(file_path):\n",
    "    return pd.read_csv(file_path, delimiter=',', header=0)\n",
    "\n",
    "# ファイルパスの指定\n",
    "file_path_msft = './data/msft.us.txt'  # 例としてMicrosoft Corp.のデータ\n",
    "\n",
    "# テキストファイルを読み込む\n",
    "stock_data_msft = read_txt_file(file_path_msft)\n",
    "\n",
    "# データの前処理\n",
    "def preprocess_data(stock_data):\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "    stock_data.set_index('Date', inplace=True)\n",
    "    stock_data.sort_index(inplace=True)\n",
    "    \n",
    "    # 必要な特徴量の計算\n",
    "    def compute_rsi(series, window=14):\n",
    "        delta = series.diff(1)\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    stock_data['SMA_20'] = stock_data['Close'].rolling(window=20).mean()  # 20日移動平均\n",
    "    stock_data['RSI'] = compute_rsi(stock_data['Close'])  # RSIの計算\n",
    "    stock_data['MACD'] = stock_data['Close'].ewm(span=12, adjust=False).mean() - stock_data['Close'].ewm(span=26, adjust=False).mean()  # MACD\n",
    "    stock_data['Upper_BB'] = stock_data['Close'].rolling(window=20).mean() + (stock_data['Close'].rolling(window=20).std() * 2)  # ボリンジャーバンド上限\n",
    "    stock_data['Lower_BB'] = stock_data['Close'].rolling(window=20).mean() - (stock_data['Close'].rolling(window=20).std() * 2)  # ボリンジャーバンド下限\n",
    "\n",
    "    # NaNを削除\n",
    "    stock_data.dropna(inplace=True)\n",
    "    return stock_data\n",
    "\n",
    "# データの前処理\n",
    "stock_data_msft = preprocess_data(stock_data_msft)\n",
    "\n",
    "# 状態空間と行動空間の定義\n",
    "states = [(sma, rsi, macd, bb_upper, bb_lower) for sma in range(20, 60, 2) for rsi in range(0, 101, 5) for macd in range(-5, 6, 1) for bb_upper in range(-5, 6, 1) for bb_lower in range(-5, 6, 1)]\n",
    "actions = [0, 1, 2]  # 0: Buy, 1: Hold, 2: Sell\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, memory_size=10000, batch_size=32):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_values = defaultdict(lambda: np.zeros(len(actions)))\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.training_errors = []\n",
    "        self.action_history = []\n",
    "        self.portfolio_values = []\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.choice(actions)\n",
    "        return np.argmax(self.q_values[state])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_values[next_state])\n",
    "        td_target = reward + self.gamma * self.q_values[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_values[state][action]\n",
    "        self.q_values[state][action] += self.alpha * td_error\n",
    "        return td_error\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state in batch:\n",
    "            self.update(state, action, reward, next_state)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# ステップ関数\n",
    "def step(state, action, stock_data, t, portfolio_value):\n",
    "    # 次の状態に遷移\n",
    "    next_state = (\n",
    "        int(stock_data.iloc[t + 1]['SMA_20']),\n",
    "        int(stock_data.iloc[t + 1]['RSI']),\n",
    "        int(stock_data.iloc[t + 1]['MACD']),\n",
    "        int(stock_data.iloc[t + 1]['Upper_BB']),\n",
    "        int(stock_data.iloc[t + 1]['Lower_BB'])\n",
    "    )\n",
    "    \n",
    "    # 報酬の計算（取引コストを考慮）\n",
    "    reward = stock_data.iloc[t + 1]['Close'] - stock_data.iloc[t]['Close']\n",
    "    if action == 0:  # Buy\n",
    "        reward -= 0.01  # 取引コスト\n",
    "        portfolio_value += stock_data.iloc[t + 1]['Close']\n",
    "    elif action == 2:  # Sell\n",
    "        reward -= 0.01  # 取引コスト\n",
    "        portfolio_value -= stock_data.iloc[t + 1]['Close']\n",
    "    \n",
    "    return next_state, reward, portfolio_value\n",
    "\n",
    "# エージェントの訓練\n",
    "agent = QLearningAgent()\n",
    "num_episodes = 10  # エピソード数を増やす\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    total_reward = 0\n",
    "    episode_length = 0\n",
    "    portfolio_value = 0  # ポートフォリオの初期価値\n",
    "    stock_data = stock_data_msft\n",
    "    state = (\n",
    "        int(stock_data.iloc[0]['SMA_20']),\n",
    "        int(stock_data.iloc[0]['RSI']),\n",
    "        int(stock_data.iloc[0]['MACD']),\n",
    "        int(stock_data.iloc[0]['Upper_BB']),\n",
    "        int(stock_data.iloc[0]['Lower_BB'])\n",
    "    )\n",
    "    for t in range(1, len(stock_data) - 1):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, portfolio_value = step(state, action, stock_data, t, portfolio_value)\n",
    "        agent.store_transition(state, action, reward, next_state)\n",
    "        total_reward += reward\n",
    "        episode_length += 1\n",
    "        state = next_state\n",
    "        agent.action_history.append(action)\n",
    "        agent.portfolio_values.append(portfolio_value)\n",
    "    agent.experience_replay()\n",
    "    agent.episode_rewards.append(total_reward)\n",
    "    agent.episode_lengths.append(episode_length)\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "# 学習結果の可視化\n",
    "def create_grids(agent):\n",
    "    state_value = defaultdict(float)\n",
    "    policy = defaultdict(int)\n",
    "    for obs, action_values in agent.q_values.items():\n",
    "        state_value[obs] = float(np.max(action_values))\n",
    "        policy[obs] = int(np.argmax(action_values))\n",
    "\n",
    "    sma_values = range(20, 60, 2)\n",
    "    rsi_values = range(0, 101, 5)\n",
    "    sma_grid, rsi_grid = np.meshgrid(sma_values, rsi_values)\n",
    "\n",
    "    value = np.apply_along_axis(\n",
    "                lambda obs: state_value[(obs[0], obs[1], 0, 0, 0)],  # MACD, BB上限, BB下限は0に固定\n",
    "        axis=2,\n",
    "        arr=np.dstack([sma_grid, rsi_grid])\n",
    "    )\n",
    "    value_grid = sma_grid, rsi_grid, value\n",
    "\n",
    "    policy_grid = np.apply_along_axis(\n",
    "        lambda obs: policy[(obs[0], obs[1], 0, 0, 0)],  # MACD, BB上限, BB下限は0に固定\n",
    "        axis=2,\n",
    "        arr=np.dstack([sma_grid, rsi_grid])\n",
    "    )\n",
    "    return value_grid, policy_grid\n",
    "\n",
    "def create_plots(value_grid, policy_grid, title: str):\n",
    "    sma_grid, rsi_grid, value = value_grid\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "    ax1.plot_surface(sma_grid, rsi_grid, value, rstride=1, cstride=1, cmap=\"viridis\", edgecolor=\"none\")\n",
    "    ax1.set_title(f\"State values: {title}\")\n",
    "    ax1.set_xlabel(\"SMA_20\")\n",
    "    ax1.set_ylabel(\"RSI\")\n",
    "    ax1.set_zlabel(\"Value\")\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"coolwarm\", cbar=False, ax=ax2)\n",
    "    ax2.set_title(f\"Policy: {title}\")\n",
    "    ax2.set_xlabel(\"SMA_20\")\n",
    "    ax2.set_ylabel(\"RSI\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "value_grid, policy_grid = create_grids(agent)\n",
    "fig = create_plots(value_grid, policy_grid, title=\"Stock Trading Strategy\")\n",
    "plt.show()\n",
    "\n",
    "# エピソードごとの報酬、エピソードの長さ、訓練エラーのプロット\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "ax1.plot(agent.episode_rewards)\n",
    "ax1.set_title('Episode Rewards')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "\n",
    "ax2.plot(agent.episode_lengths)\n",
    "ax2.set_title('Episode Lengths')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Length')\n",
    "\n",
    "ax3.plot(agent.training_errors)\n",
    "ax3.set_title('Training Error')\n",
    "ax3.set_xlabel('Step')\n",
    "ax3.set_ylabel('TD Error')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 行動選択の頻度のプロット\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "action_counts = pd.Series(agent.action_history).value_counts()\n",
    "sns.barplot(x=action_counts.index, y=action_counts.values, ax=ax)\n",
    "ax.set_title('Action Selection Frequency')\n",
    "ax.set_xlabel('Actions (0: Buy, 1: Hold, 2: Sell)')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# ポートフォリオの価値変化のプロット\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(agent.portfolio_values)\n",
    "ax.set_title('Portfolio Value Over Time')\n",
    "ax.set_xlabel('Time Steps')\n",
    "ax.set_ylabel('Portfolio Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinanceTradeEmvironment_p3119",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
